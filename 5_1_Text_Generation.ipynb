{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMRhkiDoDh6/vbrgVWboFY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavakusuma/CSV-Agent/blob/main/5_1_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation with HuggingFace - GPT2"
      ],
      "metadata": {
        "id": "kBoYE64RQy_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# . In this notebook, I will explore text generation using a GPT-2 model, which was trained to predict next words on 40GB of Internet text data."
      ],
      "metadata": {
        "id": "PjECU9LsRuTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "baKx6QYSNWrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6f8c80-4d36-46a9-d8dc-c2760ad6dc78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for reproducability\n",
        "SEED = 34\n",
        "\n",
        "#maximum number of words in output text\n",
        "MAX_LEN = 70"
      ],
      "metadata": {
        "id": "iglbeHyyPTeR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = \"I don't know about you, but there's only one thing I want to do after a long day of work\""
      ],
      "metadata": {
        "id": "GjX4Lf1gPb7f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get transformers\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "#get large GPT2 tokenizer and GPT2 model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#view model parameters\n",
        "GPT2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgO4OMk1PjYH",
        "outputId": "e20cffde-781d-4af3-acd9-c2fe32b6411e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLay  multiple                  774030080 \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 774030080 (2.88 GB)\n",
            "Trainable params: 774030080 (2.88 GB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# II. Different Decoding Methods\n",
        "First Pass (Greedy Search)\n",
        "With Greedy search, the word with the highest probability is predicted as the next word i.e. the next word is updated"
      ],
      "metadata": {
        "id": "i7YgdSm5SMD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get deep learning basics\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "KxYTk-4cQSHp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA-FscWVQYqk",
        "outputId": "4e2d0a69-60c9-4c53-c55a-2549933887c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work: go to the gym.\n",
            "\n",
            "I'm not talking about the gym that's right next to my house. I'm talking about the gym that's right next to my office.\n",
            "\n",
            "I'm not talking about the gym that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam Search with N-Gram Penalities\n",
        "#Beam search is essentially Greedy Search but the model tracks and keeps num_beams of hypotheses at each time step, so the model is able to compare alternative paths as it generates text. We can also include a n-gram penalty by setting no_repeat_ngram_size = 2 which ensures that no 2-grams appear twice. We will also set num_return_sequences = 5 so we can see what the other 5 beams looked like\n",
        "\n",
        "#To use Beam Search, we need only modify some parameters in the generate function:"
      ],
      "metadata": {
        "id": "69EEbt6gSkIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = GPT2.generate(\n",
        "    input_ids,\n",
        "    max_length = MAX_LEN,\n",
        "    num_beams = 5,\n",
        "    no_repeat_ngram_size = 2,\n",
        "    num_return_sequences = 5,\n",
        "    early_stopping = True\n",
        ")\n",
        "\n",
        "print('')\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "# now we have 3 output sequences\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTsifGdTQcRZ",
        "outputId": "4da70c4e-9037-4218-a115-2355c6c36249"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I mean, it's\n",
            "1: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a girl\n",
            "2: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a woman\n",
            "3: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a beautiful\n",
            "4: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I'm not sure if\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Sampling\n",
        "Now we will explore indeterministic decodings - sampling. Instead of following a strict path to find the end text with the highest probability, we instead randomly pick the next word by its conditional probability distribution:"
      ],
      "metadata": {
        "id": "YDUy2WIHTCvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids,\n",
        "                             do_sample = True,\n",
        "                             max_length = MAX_LEN,\n",
        "                             top_k = 0,\n",
        "                             temperature = 0.8\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZcX8oFuQfED",
        "outputId": "206d27b6-5336-4949-acf6-6a2205a64d71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work.\"\n",
            "\n",
            "\"Hmm. Must be quite the choice of words.\"\n",
            "\n",
            "\"Well, it's not a choice of words, but a need. I can't find the right answer until I find my answer.\"\n",
            "\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Top-K Sampling\n",
        "#In Top-K sampling, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high probability words occuring and decreasing the chances of low probabillity words, we just remove low probability words all together\n",
        "\n",
        "#We just need to set top_k to however many of the top words we want to consider for our conditional probability distribution:"
      ],
      "metadata": {
        "id": "HbWI3HROTFa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample from only top_k most likely words\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids,\n",
        "                             do_sample = True,\n",
        "                             max_length = MAX_LEN,\n",
        "                             top_k = 50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O913eL3lQiJc",
        "outputId": "c01d997e-8e60-40e9-9afe-1c1e2be09991"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work: sit down for long, uninterrupted hours in front of a computer and play a game! And today, it turns out, that's exactly what I'll do. It's a new Android game called Pocket Heroes' World by Pocket Heroes ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Top-P Sampling¶\n",
        "#Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely wordsm we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set\n",
        "\n",
        "#The main difference here is that with Top-K sampling, the size of the set of words is static (obviously) whereas in Top-P sampling, the size of the set can change. To use this sampling method, we just set top_k = 0 and choose a value top_p:"
      ],
      "metadata": {
        "id": "KIyy4GG9TS9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample only from 80% most likely words\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids,\n",
        "                             do_sample = True,\n",
        "                             max_length = MAX_LEN,\n",
        "                             top_p = 0.8,\n",
        "                             top_k = 0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DZ2GOHfRN1P",
        "outputId": "17173645-a804-4b25-8262-9bc6880a5103"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work: drink a beer. I don't have to be able to read and write to drink one. My parents taught me to do that before I was four, and I think I got the hang of it pretty well.\n",
            "\n",
            "Advertisement ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Top-K and Top-P Sampling\n",
        "#As you could have probably guessed, we can use both Top-K and Top-P sampling here. This reduces the chances of us getting weird words (low probability words) while allowing for a dynamic selection size. We need only top a value for both top_k and top_p. We can even include the inital temperature parameter if we want to, Let's now see how our model performs now after adding everything together. We will check the top 5 return to see how diverse our answers are:"
      ],
      "metadata": {
        "id": "XIZjG6E1ThSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine both sampling techniques                                               #to test how long we can generate and it be coherent\n",
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = 2*MAX_LEN,\n",
        "                              #temperature = .7,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85,\n",
        "                              num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzXvaX9URQ7q",
        "outputId": "6e5f883f-bf6e-4fed-ba25-e30c5eebe63f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I don't know about you, but there's only one thing I want to do after a long day of work, and that is go to the bathroom.\" The only thing I want to do after a long day of work, and that is go to the bathroom.\n",
            "\n",
            "What is your goal as an author? Is it just to write better books?\n",
            "\n",
            "To write better books. Writing is about the journey, the creation, the writing. It's not about making it easy, or finding a good format, it's about the journey. There are two main things you can do when you write a book, and that's the writing of the book itself and then you have to have...\n",
            "\n",
            "1: I don't know about you, but there's only one thing I want to do after a long day of work. I want to watch a movie. And since I'm here in America, I just had to take a road trip. It's not my usual choice, but I couldn't get my own way. I'm on my own. That's why I'm here.\"\n",
            "\n",
            "\"That's not the only reason, of course, but you're the one who came up with the idea,\" a man with a deep voice said as he walked up to me.\n",
            "\n",
            "\"Yes, sir,\" I answered. \"I think I'd like to try it.\"\n",
            "\n",
            "\"The...\n",
            "\n",
            "2: I don't know about you, but there's only one thing I want to do after a long day of work: to walk the streets, get a bite to eat and have a chat with my friends. So I took a stroll down Market Street in a random direction, trying to pick up a couple of new acquaintances.\n",
            "\n",
            "I was pleasantly surprised when I found a cute, friendly-looking couple, chatting away. They came up to me to introduce themselves and asked if I wanted to go on a walk.\n",
            "\n",
            "\"Why not?\" I asked.\n",
            "\n",
            "\"Because it's hot,\" the woman said.\n",
            "\n",
            "I smiled and nodded. \"It is,\" I said.\n",
            "...\n",
            "\n",
            "3: I don't know about you, but there's only one thing I want to do after a long day of work: drink a few more beers and get down to the next level.\"\n",
            "\n",
            "A number of companies have also started offering free drinks to employees as a way to keep morale high. At LinkedIn, for example, the company offers employees free beer on the weekends as a way to keep morale high and increase work-life balance.\n",
            "\n",
            "What do you think? Would you be willing to give employees free beer during the week? Let us know your thoughts in the comments below!\n",
            "\n",
            "Image via Flickr user David Rabinovitz...\n",
            "\n",
            "4: I don't know about you, but there's only one thing I want to do after a long day of work: go to a bar.\"\n",
            "\n",
            "I nodded. \"The rest of us would be happy to join you there.\"\n",
            "\n",
            "\"That would be the one thing I'm happy to do. But no, I'm not doing that. I'll be back in another three minutes.\"\n",
            "\n",
            "I stared at him for a moment. \"Wait, are you saying…?\"\n",
            "\n",
            "\"Yes. I am, if you want to know the truth.\" He looked at me like I was insane. \"You know, when I told you to tell me that I was going to go...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 150"
      ],
      "metadata": {
        "id": "hyIDLNO0RVb6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\n",
        "\n",
        "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
      ],
      "metadata": {
        "id": "LOoi8nzjRZMV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .8,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85\n",
        "                              #num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbREHAsmRb84",
        "outputId": "fee12849-ff38-4971-9adc-b1fabcb22610"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "A study by University of the Andes researcher Francisco Cisneros and colleagues has found evidence that unicorns speak a perfect version of English that humans can understand. The researchers discovered that the wild unicorns communicate by using only the four basic sounds – \"toot\" for a click, \"chow\" for a swallow and \"wah\" for a woof. This suggests that they have developed a language to communicate in, which we cannot speak.\n",
            "\n",
            "Researchers believe that these unicorns were raised in a...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\n",
        "\n",
        "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
      ],
      "metadata": {
        "id": "tALYlZTARcu3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .8,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85\n",
        "                              #num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')# . In this notebook, I will explore text generation using a GPT-2 model, which was trained to predict next words on 40GB of Internet text data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqjsnqyORff8",
        "outputId": "73145218-8b5b-4f83-942d-2f5633174baf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. As they approached, they were immediately met with the first orc attack of the day, a small number of orcs charged towards them, their arrows piercing through their shields. The battle was short-lived, however, as the two hobbits quickly disarmed and brought down the charging warriors. As a result of their quick thinking, they were able to kill two of the orc warlords before they were able to kill their own. After the battle, the hobbits had killed over 50 orcs in a single battle. With a score of around 100 kills, the two hobbits had earned the rank of Grand Champion.\n",
            "\n",
            "Afterwards, the two hobbits...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n",
        "\n",
        "input_ids = tokenizer.encode(prompt3, return_tensors='tf')"
      ],
      "metadata": {
        "id": "avJGfwm9TxMI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .8,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85\n",
        "                              #num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqCinjo5Txw1",
        "outputId": "e7c1498f-bd9d-444c-915d-e6cbd0d1fd95"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. Behind them, the orcs raised their own weapons and fought. But before the battle could be waged, a huge, terrible beast had appeared. It roared and bellowed and roared, and the battle was over.\n",
            "\n",
            "The orc that had fought the trolls had been slain. But the orc who had fought the trolls had not been slain. The orcs fought and battled with their weapons and their spells, but the creature that had attacked them was not the orc they had been fighting. The creature was not even a troll. The orc was something much more horrible than the trolls. And it was coming toward them.\n",
            "\n",
            "The battle between the orcs and the...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSq4tiK6WUR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}